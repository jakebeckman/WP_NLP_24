{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Topic Modeling Assessment Project\n",
    "\n",
    "Welcome to your Topic Modeling Assessment! For this project you will be working with a dataset of over 400,000 quora questions that have no labeled cateogry, and attempting to find 20 cateogries to assign these questions to. The .csv file of these text questions can be found underneath the Topic-Modeling folder.\n",
    "\n",
    "Remember you can always check the solutions notebook and video lecture for any questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Import pandas and read in the quora_questions.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\blake.coston\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x188bb931fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('quora_questions.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "#### Task: Use TF-IDF Vectorization to create a vectorized document term matrix. You may want to explore the max_df and min_df parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Get the default English stopwords from spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "    \n",
    "# Function to preprocess, tokenize, and lemmatize text\n",
    "def preprocess(text):\n",
    "    tokens = text.split()\n",
    "    return ' '.join([lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words])\n",
    "\n",
    "# Apply the preprocessing to the questions\n",
    "df['preprocessed_question'] = df['Question'].apply(lambda text: preprocess(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data\n",
    "# max_df: if a word is contianed in 95% of documents it is discarded\n",
    "# min_df: if a word is only contained in 2 or less documents it is discarded\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 2, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['preprocessed_question'].fillna(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>preprocessed_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>step step guide invest share market india?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>story kohinoor (koh-i-noor) diamond?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>increase speed internet connection vpn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>mentally lonely? solve it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>dissolve water quikly sugar, salt, methane car...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                               preprocessed_question  \n",
       "0         step step guide invest share market india?  \n",
       "1               story kohinoor (koh-i-noor) diamond?  \n",
       "2            increase speed internet connection vpn?  \n",
       "3                         mentally lonely? solve it?  \n",
       "4  dissolve water quikly sugar, salt, methane car...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-negative Matrix Factorization\n",
    "\n",
    "#### TASK: Using Scikit-Learn create an instance of NMF with 20 expected components. (Use random_state=42).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NMF(n_components=20, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(n_components=20, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NMF(n_components=20, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=20, random_state=42)\n",
    "\n",
    "nmf.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TASK: Print our the top 15 most common words for each of the 20 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "['2016', 'best', 'book', 'buy', 'company', 'friend', 'laptop', 'movie', 'movies', 'phone', 'place', 'read', 'site', 'visit', 'website']\n",
      "Topic 2\n",
      "['add', 'answer', 'answered', 'ask', 'asked', 'delete', 'easily', 'google', 'improvement', 'marked', 'needing', 'post', 'question', 'quora', 'search']\n",
      "Topic 3\n",
      "['black', 'earn', 'easily', 'easy', 'facebook', 'free', 'home', 'internet', 'investment', 'job', 'making', 'money', 'online', 'website', 'youtube']\n",
      "Topic 4\n",
      "['balance', 'change', 'changed', 'death', 'earth', 'important', 'life', 'live', 'meaning', 'moment', 'purpose', 'real', 'thing', 'want', 'work']\n",
      "Topic 5\n",
      "['2016', 'america', 'better', 'clinton', 'donald', 'election', 'happen', 'hillary', 'presidency', 'president', 'presidential', 'think', 'trump', 'vote', 'win']\n",
      "Topic 6\n",
      "['culture', 'don', 'feel', 'friend', 'girl', 'guy', 'like', 'live', 'look', 'men', 'sex', 'tell', 'website', 'woman', 'work']\n",
      "Topic 7\n",
      "['beginner', 'book', 'coding', 'computer', 'english', 'hacking', 'java', 'language', 'learn', 'learning', 'online', 'programming', 'python', 'start', 'want']\n",
      "Topic 8\n",
      "['available', 'business', 'china', 'college', 'company', 'country', 'india', 'job', 'minister', 'olympics', 'pakistan', 'place', 'president', 'spotify', 'war']\n",
      "Topic 9\n",
      "['ask', 'believe', 'blowing', 'don', 'earth', 'easily', 'flat', 'google', 'hate', 'instagram', 'mind', 'people', 'stop', 'think', 'use']\n",
      "Topic 10\n",
      "['account', 'best', 'commit', 'easiest', 'easy', 'fastest', 'increase', 'instagram', 'learn', 'painless', 'quickest', 'rid', 'stop', 'suicide', 'way']\n",
      "Topic 11\n",
      "['blowing', 'day', 'don', 'employee', 'going', 'happened', 'important', 'know', 'mind', 'new', 'pregnant', 'thing', 've', 'want', 'worst']\n",
      "Topic 12\n",
      "['bad', 'balance', 'book', 'college', 'departments', 'differ', 'engineering', 'good', 'idea', 'job', 'position', 'read', 'song', 'website', 'work']\n",
      "Topic 13\n",
      "['1000', '2000', '500', 'ban', 'banning', 'black', 'currency', 'economy', 'government', 'indian', 'modi', 'note', 'notes', 'rupee', 'think']\n",
      "Topic 14\n",
      "['ability', 'communication', 'english', 'fluent', 'fluently', 'improve', 'language', 'pronunciation', 'skill', 'skills', 'speak', 'speaking', 'spoken', 'word', 'writing']\n",
      "Topic 15\n",
      "['boyfriend', 'doesn', 'fall', 'forget', 'friend', 'girl', 'girlfriend', 'guy', 'know', 'love', 'man', 'mean', 'person', 'tell', 'true']\n",
      "Topic 16\n",
      "['day', 'favorite', 'feel', 'home', 'job', 'long', 'machine', 'movie', 'person', 'possible', 'sex', 'spend', 'time', 'travel', 'visitor']\n",
      "Topic 17\n",
      "['belly', 'diet', 'exercise', 'fast', 'fat', 'gain', 'healthy', 'help', 'lose', 'loss', 'month', 'pound', 'quickly', 'reduce', 'weight']\n",
      "Topic 18\n",
      "['account', 'chinese', 'computer', 'culture', 'data', 'difference', 'engineering', 'java', 'main', 'major', 'science', 'similarity', 'software', 'use', 'western']\n",
      "Topic 19\n",
      "['2017', 'business', 'engineering', 'grad', 'job', 'looking', 'major', 'new', 'old', 'prepare', 'resolution', 'start', 'university', 'want', 'year']\n",
      "Topic 20\n",
      "['better', 'cause', 'coming', 'country', 'end', 'going', 'happen', 'iii', 'live', 'pakistan', 'place', 'russia', 'think', 'war', 'world']\n"
     ]
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names_out()\n",
    "for i, component in enumerate(nmf.components_):\n",
    "    ordered = sorted(component, reverse=True)\n",
    "    print(f'Topic {i + 1}')\n",
    "    top_words = []\n",
    "    for word_index, num in enumerate(component):\n",
    "        if num in ordered[:15]:\n",
    "            top_words.append(words[word_index])\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TASK: Add a new column to the original quora dataframe that labels each question into one of the 20 topic categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question\n",
       "0  What is the step by step guide to invest in sh...\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...\n",
       "2  How can I increase the speed of my internet co...\n",
       "3  Why am I mentally very lonely? How can I solve...\n",
       "4  Which one dissolve in water quikly sugar, salt..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  Topic\n",
       "0  What is the step by step guide to invest in sh...      5\n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...     16\n",
       "2  How can I increase the speed of my internet co...     17\n",
       "3  Why am I mentally very lonely? How can I solve...     11\n",
       "4  Which one dissolve in water quikly sugar, salt...     14\n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...      1\n",
       "6                                Should I buy tiago?      0\n",
       "7                     How can I be a good geologist?     10\n",
       "8                    When do you use シ instead of し?     19\n",
       "9  Motorola (company): Can I hack my Charter Moto...     17"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Methods of Grouping\n",
    "K-means and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster_k_means 0 keywords: ['mean', 'say', 'dream', 'girl', 'symbol', 'guy', 'word', 'phrase', 'love', 'person']\n",
      "cluster_k_means 1 keywords: ['long', 'school', 'distance', 'relationship', 'high', 'work', 'term', 'best', 'stay', 'time']\n",
      "cluster_k_means 2 keywords: ['like', 'people', 'feel', 'think', 'look', 'girl', 'work', 'culture', 'sex', 'hate']\n",
      "cluster_k_means 3 keywords: ['attend', 'like', 'school', 'university', 'college', 'conference', 'international', 'student', 'yale', 'best']\n",
      "cluster_k_means 4 keywords: ['buy', 'best', 'laptop', 'phone', '15000', 'car', 'india', 'online', '15k', 'inr']\n",
      "cluster_k_means 5 keywords: ['account', 'password', 'gmail', 'email', 'facebook', 'instagram', 'recover', 'delete', 'hack', 'forgot']\n",
      "cluster_k_means 6 keywords: ['computer', 'science', 'engineering', 'data', 'best', 'university', 'difference', 'student', 'good', 'learning']\n",
      "cluster_k_means 7 keywords: ['whatsapp', 'hack', 'message', 'account', 'phone', 'group', 'profile', 'chat', 'status', 'know']\n",
      "cluster_k_means 8 keywords: ['stop', 'thinking', 'worrying', 'caring', 'people', 'masturbation', 'think', 'masturbating', 'feeling', 'mix']\n",
      "cluster_k_means 9 keywords: ['quora', 'question', 'answer', 'ask', 'people', 'improvement', 'asked', 'google', 'easily', 'needing']\n",
      "cluster_k_means 10 keywords: ['use', 'jio', 'quora', 'sim', 'people', 'phone', 'sentence', '3g', 'google', 'instead']\n",
      "cluster_k_means 11 keywords: ['know', 'love', 'thing', 'employee', 'going', 'don', 'people', 'new', 'day', 'fall']\n",
      "cluster_k_means 12 keywords: ['weight', 'lose', 'gain', 'way', 'best', 'loss', 'fat', 'pound', 'fast', 'reduce']\n",
      "cluster_k_means 13 keywords: ['language', 'programming', 'learn', 'best', 'english', 'learning', 'start', 'improve', 'beginner', 'scripting']\n",
      "cluster_k_means 14 keywords: ['learn', 'best', 'way', 'english', 'book', 'programming', 'java', 'python', 'online', 'hacking']\n",
      "cluster_k_means 15 keywords: ['iphone', 'buy', 'plus', 'icloud', 'apple', '5s', 'app', '6s', 'photo', 'worth']\n",
      "cluster_k_means 16 keywords: ['best', 'way', 'book', 'india', 'movie', 'place', '2016', 'online', 'laptop', 'site']\n",
      "cluster_k_means 17 keywords: ['compare', 'average', 'desert', 'battle', 'temperature', 'earthquake', 'sahara', 'effect', 'contrast', 'cambodia']\n",
      "cluster_k_means 18 keywords: ['example', 'sentence', 'presence', 'word', 'mind', 'life', 'real', 'good', 'great', 'karma']\n",
      "cluster_k_means 19 keywords: ['india', 'good', 'life', 'difference', 'money', 'time', 'trump', 'indian', 'thing', 'way']\n",
      "\n",
      "Cluster counts:\n",
      "Cluster\n",
      "0       4360\n",
      "1       4876\n",
      "2      18880\n",
      "3        188\n",
      "4       2905\n",
      "5       3893\n",
      "6       2700\n",
      "7       1327\n",
      "8       2812\n",
      "9       8591\n",
      "10      5111\n",
      "11      7935\n",
      "12      3558\n",
      "13      3035\n",
      "14      4958\n",
      "15      2063\n",
      "16     28326\n",
      "17      3591\n",
      "18      2222\n",
      "19    292958\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# K-means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_and_filter_relevance(df, n_clusters=5, n_key_words=10):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on the articles' abstracts and filter the most relevant clusters with lemmatization.\n",
    "    USes TF-IDF to vectorize the documents and K-means to cluster them into groups\n",
    "    \n",
    "    :param df: DataFrame containing the articles data.\n",
    "    :param n_clusters: Number of clusters to create.\n",
    "    :param n_key_words: Number of top keywords to use for filtering relevant clusters.\n",
    "    \n",
    "    :return: Filtered DataFrame with relevant clusters, and a dictionary containing cluster keywords.\n",
    "    \"\"\"\n",
    "    # Vectorize the text data\n",
    "    # max_df: if a word is contianed in 95% of documents it is discarded\n",
    "    # min_df: if a word is only contained in 2 or less documents it is discarded\n",
    "    vectorizer = TfidfVectorizer(max_df = 0.95, min_df = 2, stop_words='english')\n",
    "    X = vectorizer.fit_transform(df['Preprocessed_Question'].fillna(''))\n",
    "\n",
    "    # Perform K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X)\n",
    "    df['K_Means_Cluster'] = kmeans.labels_\n",
    "    \n",
    "    # Analyze the clusters to determine relevance\n",
    "    # Initialize a dictionary to store the keywords for each cluster\n",
    "    cluster_keywords = {}\n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Iterate through each cluster and store the top n keywords\n",
    "    for i in range(n_clusters):\n",
    "        cluster_keywords[i] = [terms[ind] for ind in order_centroids[i, :n_key_words]]\n",
    "    \n",
    "    # Here you could filter clusters based on relevance, or simply drop the combined column\n",
    "    df_filtered = df.copy()  # If you want to perform further filtering, modify df_filtered\n",
    "\n",
    "    # Return both the filtered DataFrame and the cluster_keywords dictionary\n",
    "    return df_filtered, cluster_keywords\n",
    "\n",
    "df_k_means, clusters_k_means = cluster_and_filter_relevance(df, 20, 10)\n",
    "\n",
    "for group in sorted(clusters_k_means.keys()):\n",
    "    print(f'cluster_k_means {group} keywords: {clusters_k_means[group]}')\n",
    "\n",
    "print(\"\\nCluster counts:\")\n",
    "print(df_k_means['Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA (takes a long time)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# Use CountVectorizer to convert the text data into a matrix of token counts\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Preprocessed_Question'])\n",
    "\n",
    "# Define the LDA model with the number of topics you want to extract\n",
    "num_topics = 20  # You can adjust this number\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Get the topic distribution for each document\n",
    "doc_topic_dist = lda.transform(X)\n",
    "\n",
    "# Reduce the dimensionality of the topics for visualization using t-SNE\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "tsne_lda = tsne_model.fit_transform(doc_topic_dist)\n",
    "\n",
    "# Plot the topics in a 2D space\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(tsne_lda[:, 0], tsne_lda[:, 1], c=doc_topic_dist.argmax(axis=1), cmap='viridis')\n",
    "plt.colorbar(label='Topic')\n",
    "plt.title('2D Visualization of LDA Topics')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.show()\n",
    "\n",
    "# Display the top words in each topic\n",
    "num_top_words = 10  # Number of top words to display for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
    "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
